# NLP_PAPER_STUDY
Natural Language Processing Paper Study

# Paper-Study

#### 발표자료 정리

|       Date       | Week | Topic | Presenters | Slides |
|:----------------:|:------:|:----------------------------------------:|:----------:|:------:|
| 2020.01.29 | Week12 | Reformer, the Efficient Transformer | 김강우 | [Slides](https://github.com/minstar/NLP_PAPER_STUDY/blob/master/Paper-PPT/Reformer,_the_Efficient_Transformer.pdf) |
| 2020.01.22 | Week11 | Data dependent Gaussian Prior Objective for Language Generation | 고미영 | [Slides](https://github.com/minstar/NLP_PAPER_STUDY/blob/master/Paper-PPT/Data_dependent_Gaussian_Prior_Objective_for_Language_Generation.pdf) |
| 2020.01.22 | Week11 | Probing Neural Network Comprehension of Natural Language Arguements | 이잉걸 | [Slides](https://github.com/minstar/NLP_PAPER_STUDY/blob/master/Paper-PPT/Probing_Neural_Network_Comprehension_of_Natural_Language_Arguements.pdf) |
| 2020.01.15 | Week10 | Generalization through Memorization : Nearest Neighbor Language Models | 김현재 | [Slides](https://github.com/minstar/NLP_PAPER_STUDY/blob/master/Paper-PPT/Generalization_through_Memorization,Nearest_Neighbor_Language_Models.pdf) |
| 2020.01.08 | Week9 | Specializing Word Embeddings (for Parsing) by Information Bottleneck | 김강우 | [Slides](https://github.com/minstar/NLP_PAPER_STUDY/blob/master/Paper-PPT/SpecializingWord_Embeddings_(for_Parsing)_by_Information_Bottleneck.pdf) |
| 2020.01.08 | Week9 | Meta Learning with Memory Augmented Neural Networks | 박정수 | [Slides](https://github.com/minstar/NLP_PAPER_STUDY/blob/master/Paper-PPT/Meta_Learning_with_Memory_Augmented_Neural_Networks.pdf) |
| 2020.01.02 | Week8 | Dice Loss for Data imbalanced NLP Tasks | 정민별 | [Slides](https://github.com/minstar/NLP_PAPER_STUDY/blob/master/Paper-PPT/Dice_Loss_for_Data_imbalanced_NLP_Tasks.pdf) |
| 2020.01.02 | Week8 | Poincare Glove: Hyperbolic Word Embeddings | 이잉걸 | [Slides](https://github.com/minstar/NLP_PAPER_STUDY/blob/master/Paper-PPT/POINCAR%20%CC%81E_GLOVE_HYPERBOLICWORD_EMBEDDINGS.pdf) |
| 2019.12.11 | Week7 | Assessing the Benchmarking Capacity of Machine Reading Comprehension | 정민별 | [Slides](https://github.com/minstar/NLP_PAPER_STUDY/blob/master/Paper-PPT/Assessing_the_Benchmarking_Capacity_of_Machine_Reading_Comprehension_Datasets.pdf) |
| 2019.11.27 | Week5 | Adversarial Removal of Demographic Attributes from Textual Data | 김강우 | [Slides](https://github.com/minstar/NLP_PAPER_STUDY/blob/master/Paper-PPT/Adversarial_removal_of_demographic_attributes_from_textual_data.pdf) |
| 2019.11.20 | Week4 | Unsupervised Domain Adaptation on Reading Comprehension | 정민별 | [Slides](https://github.com/minstar/NLP_PAPER_STUDY/blob/master/Paper-PPT/Unsupervised_Domain_Adaptation_on_Reading_Comprehension.pdf) |
| 2019.11.20 | Week4 | Aspect-based Sentiment Classification with Graph Convolutional Networks | 김현재 | [Slides](https://github.com/minstar/NLP_PAPER_STUDY/blob/master/Paper-PPT/Aspect-based_Sentiment_Classification_with_Graph_Convolutional_Networks.pdf) |
| 2019.11.15 | Week3 | Poincaré Embeddings for Learning Hierarchical Representations | 이잉걸 | [Slides](https://github.com/minstar/NLP_PAPER_STUDY/blob/master/Paper-PPT/Poincar%C3%A9_Embeddings_for_Learning_Hierarchical_Representations.pdf) |
| 2019.11.08 | Week2 | A Discrete Hard EM Approach for Weakly Supervised Question Answering | 정민별 | [Slides](https://github.com/minstar/NLP_PAPER_STUDY/blob/master/Paper-PPT/A_Discrete_Hard_EM_Approach_for_Weakly_Supervised_Question_Answering.pdf) |
| 2019.11.08 | Week2 | Visualizing and Measuring the Geometry of BERT | 김강우 | [Slides](https://github.com/minstar/NLP_PAPER_STUDY/blob/master/Paper-PPT/Visualizing_and_Measuring_the_Geometry_of_BERT.pdf) |
| 2019.11.01 | Week1 | Neural Text Generation With Unlikelihood Training | 박정수 | [Slides](https://github.com/minstar/NLP_PAPER_STUDY/blob/master/Paper-PPT/Neural_Text_Generation_With_Unlikelihood_Training.pdf) |
| 2019. | Week0 | BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding | 박정수 | [Slides](https://github.com/minstar/NLP_PAPER_STUDY/blob/master/Paper-PPT/BERT.pdf) |
| 2019. | Week0 | Are All Languages Equally Hard to Language-Model? | 박정수 | [Slides](https://github.com/minstar/NLP_PAPER_STUDY/blob/master/Paper-PPT/Are_All_Languages_Equally_Hard_to_Language_Models.pdf) |
| 2019. | Week0 | Neural Discrete Representation Learning | 박정수 | [Slides](https://github.com/minstar/NLP_PAPER_STUDY/blob/master/Paper-PPT/Neural_Discrete_Represenation_Learning.pdf) |
| 2019. | Week0 | Robust Neural Machine Translation with Doubly Adversarial Inputs | 박정수 | [Slides](https://github.com/minstar/NLP_PAPER_STUDY/blob/master/Paper-PPT/Robust_Neural_Machine_Translation_with_Doubly_Adversarial_Inputs.pdf) |
| 2019. | Week0 | Style Transfer from Non-Parallel Text by Cross-Alignment | 박정수 | [Slides](https://github.com/minstar/NLP_PAPER_STUDY/blob/master/Paper-PPT/Style_Transfer_from_Non_Parallel_Text_by_%20Cross_Alignment.pdf) |
| 2019. | Week0 | Theory and Experiments on Vector Quantized Autoencoders | 박정수 | [Slides](https://github.com/minstar/NLP_PAPER_STUDY/blob/master/Paper-PPT/Theory_and_Experiments_on_Vector_Quantized_Autoencoders.pdf) |
| 2019. | Week0 | VON MISES-FISHER LOSS FOR TRAINING SEQUENCE TO SEQUENCE MODELS WITH CONTINUOUS OUTPUTS | 박정수 | [Slides](https://github.com/minstar/NLP_PAPER_STUDY/blob/master/Paper-PPT/Von_Mises_Fisher_Loss_for_Training_Sequence_to_Sequence_Models_with_Continuous_Outputs.pdf) |


#### 참고자료 정리

##### Week-12 (2020.01.29)
* Reformer, the Efficient Transformer (김강우)))

  [Reformer, the Efficient Transformer](https://arxiv.org/abs/2001.04451)

##### Week-11 (2020.01.22)
* Data dependent Gaussian Prior Objective for Language Generation (고미영)))

  [Data dependent Gaussian Prior Objective for Language Generation](https://openreview.net/pdf?id=S1efxTVYDr)

* Probing Neural Network Comprehension of Natural Language Arguements (이잉걸)))

  [Probing Neural Network Comprehension of Natural Language Arguements](https://arxiv.org/abs/1907.07355)

##### Week-10 (2020.01.15)
* Generalization through Memorization : Nearest Neighbor Language Models (김현재)))

  [Generalization through Memorization : Nearest Neighbor Language Models](https://arxiv.org/abs/1911.00172)

##### Week-9 (2020.01.08)
* Specializing Word Embeddings (for Parsing) by Information Bottleneck (김강우))

  [Specializing Word Embeddings (for Parsing) by Information Bottleneck](https://arxiv.org/abs/1910.00163)
  
* Meta Learning with Memory Augmented Neural Networks (박정수))

  [Meta Learning with Memory Augmented Neural Networks](https://arxiv.org/abs/1605.06065)

  
##### Week-8 (2020.01.02)
* Dice Loss for Data imbalanced NLP Tasks (정민별))

  [Dice Loss for Data imbalanced NLP Tasks](https://arxiv.org/pdf/1911.02855.pdf)

* Poincare Glove: Hyperbolic Word Embeddings (이잉걸))

  [Poincare Glove: Hyperbolic Word Embeddings (ICLR 2019)](https://arxiv.org/pdf/1810.06546.pdf)


##### Week-7 (2019.12.11)
* Assessing the Benchmarking Capacity of Machine Reading Comprehension Datasets (정민별))

  [Assessing the Benchmarking Capacity of Machine Reading Comprehension Datasets (AAAI 2020)](https://arxiv.org/pdf/1911.09241.pdf)

##### Week-5 (2019.11.27)
* Adversarial Removal of Demographic Attributes from Textual Data (김강우))

  [Adversarial Removal of Demographic Attributes from Textual Data (EMNLP 2018)](https://www.aclweb.org/anthology/D18-1002.pdf)

##### Week-4 (2019.11.20)
* Aspect-based Sentiment Classification with Graph Convolutional Networks (김현재)

  [Aspect-based Sentiment Classification with Aspect-specific Graph Convolutional Networks (EMNLP 2019)](https://arxiv.org/abs/1909.03477)
  
  [Syntax-Aware Aspect Level Sentiment Classification with Graph Attention Networks (EMNLP 2019)
](https://arxiv.org/abs/1909.02606)

* Unsupervised Domain Adaptation on Reading Comprehension (정민별)

  [Unsupervised Domain Adaptation on Reading Comprehension](https://arxiv.org/pdf/1911.06137.pdf)

##### Week-3 (2019.11.15)
* Poincaré Embeddings for Learning Hierarchical Representations (이잉걸)

  [Poincaré Embeddings for Learning Hierarchical Representations (NIPS 2017)](https://papers.nips.cc/paper/7213-poincare-embeddings-for-learning-hierarchical-representations.pdf)

##### Week-2 (2019.11.08)
* A Discrete Hard EM Approach for Weakly Supervised Question Answering (정민별)

  [A Discrete Hard EM Approach for Weakly Supervised Question Answering (EMNLP 2019)](https://arxiv.org/pdf/1909.04849.pdf)

* Visualizing and Measuring the Geometry of BERT (김강우)

  [Visualizing and Measuring the Geometry of BERT (NIPS 2019)](https://arxiv.org/pdf/1906.02715.pdf)

##### Week-1 (2019.11.01)
* Neural Text Generation With Unlikelihood Training (박정수)

  [Neural Text Generation With Unlikelihood Training](https://arxiv.org/pdf/1908.04319.pdf)

##### Week-0 (2019.)
* BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (박정수)

  [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)

* Are All Languages Equally Hard to Language-Model? (박정수)

  [Are All Languages Equally Hard to Language-Model?](https://ryancotterell.github.io/papers/cotterell+alc.naacl18.pdf)
  
* Neural Discrete Representation Learning (박정수)

  [Neural Discrete Representation Learning](https://arxiv.org/pdf/1711.00937.pdf)

* Robust Neural Machine Translation with Doubly Adversarial Inputs (박정수)

  [Robust Neural Machine Translation with Doubly Adversarial Inputs](https://arxiv.org/pdf/1906.02443.pdf)

* Style Transfer from Non-Parallel Text by Cross-Alignment (박정수)

  [Style Transfer from Non-Parallel Text by Cross-Alignment](https://papers.nips.cc/paper/7259-style-transfer-from-non-parallel-text-by-cross-alignment.pdf)

* Theory and Experiments on Vector Quantized Autoencoders (박정수)

  [Theory and Experiments on Vector Quantized Autoencoders](https://arxiv.org/pdf/1805.11063.pdf)

* VON MISES-FISHER LOSS FOR TRAINING SEQUENCE TO SEQUENCE MODELS WITH CONTINUOUS OUTPUTS (박정수)

  [VON MISES-FISHER LOSS FOR TRAINING SEQUENCE TO SEQUENCE MODELS WITH CONTINUOUS OUTPUTS](https://arxiv.org/pdf/1812.04616.pdf)

